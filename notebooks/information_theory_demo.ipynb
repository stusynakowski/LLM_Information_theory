{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7641e7a",
   "metadata": {},
   "source": [
    "# LLM Information Theory Analysis\n",
    "\n",
    "This notebook demonstrates how to analyze information-theoretic properties of language model generation, including:\n",
    "1. **Probability Extraction**: Access token probabilities from the decoder\n",
    "2. **Intervention**: Modify generation process in real-time\n",
    "3. **Information Theory**: Compute entropy and Shannon information for each token\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import our custom modules and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b191b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# Import our modules\n",
    "from probability_extractor import ProbabilityExtractor\n",
    "from information_theory import (\n",
    "    compute_entropy,\n",
    "    compute_shannon_information,\n",
    "    analyze_token_information,\n",
    "    compute_varentropy\n",
    ")\n",
    "from intervention import InterventionManager\n",
    "\n",
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952184a",
   "metadata": {},
   "source": [
    "## 1. Load Your Local Llama Model\n",
    "\n",
    "Update the model name to point to your local Llama model. You can use:\n",
    "- HuggingFace model names: `\"meta-llama/Llama-2-7b-hf\"`\n",
    "- Local paths: `\"/path/to/your/model\"`\n",
    "- Quantized versions for lower memory usage (set `load_in_8bit=True` or `load_in_4bit=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Change this to your model\n",
    "\n",
    "# For smaller/quantized models:\n",
    "# extractor = ProbabilityExtractor(MODEL_NAME, load_in_8bit=True)\n",
    "# extractor = ProbabilityExtractor(MODEL_NAME, load_in_4bit=True)\n",
    "\n",
    "# Load the model\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "extractor = ProbabilityExtractor(\n",
    "    MODEL_NAME,\n",
    "    device=None,  # Auto-detect CUDA/CPU\n",
    "    load_in_8bit=False  # Set to True if you have limited VRAM\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54125d6",
   "metadata": {},
   "source": [
    "## 2. Extracting Token Probabilities\n",
    "\n",
    "Let's examine what the model predicts for the next token and see the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de395612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Get next token probabilities\n",
    "result = extractor.get_next_token_probabilities(prompt, return_top_k=15)\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"Top 15 most likely next tokens:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (token, prob) in enumerate(result['top_k_tokens'], 1):\n",
    "    print(f\"{i:2d}. '{token}' - Probability: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "# Calculate entropy of the distribution\n",
    "entropy = compute_entropy(result['probabilities'])\n",
    "print(f\"\\nEntropy of next token distribution: {entropy:.4f} bits\")\n",
    "print(f\"Varentropy: {compute_varentropy(result['probabilities']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83c8743",
   "metadata": {},
   "source": [
    "### Visualize the Probability Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top-k probabilities\n",
    "tokens, probs = zip(*result['top_k_tokens'][:10])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(tokens)), probs, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Token', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.title(f'Top 10 Next Token Probabilities for: \"{prompt}\"', fontsize=14)\n",
    "plt.xticks(range(len(tokens)), [t.replace(' ', '␣') for t in tokens], rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1984206",
   "metadata": {},
   "source": [
    "## 3. Generate Text with Full Probability Tracking\n",
    "\n",
    "Generate a sequence and track the probability of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0537add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with probability tracking\n",
    "generation_prompt = \"The theory of relativity states that\"\n",
    "\n",
    "result = extractor.generate_with_probabilities(\n",
    "    prompt=generation_prompt,\n",
    "    max_new_tokens=30,\n",
    "    temperature=0.8,\n",
    "    return_full_distribution=False  # Set to True if you want full distributions\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {generation_prompt}\")\n",
    "print(f\"\\nGenerated text:\\n{result['generated_text']}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Display each token with its probability\n",
    "print(\"Token-by-token breakdown:\")\n",
    "print(\"-\" * 70)\n",
    "for i, (token, prob) in enumerate(zip(result['generated_tokens'], result['token_probabilities']), 1):\n",
    "    surprisal = compute_shannon_information(prob)\n",
    "    print(f\"{i:2d}. '{token:15s}' | p={prob:.4f} | surprisal={surprisal:.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997efbd3",
   "metadata": {},
   "source": [
    "## 4. Information Theory Analysis\n",
    "\n",
    "Compute entropy and Shannon information for the generated sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbf883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive information analysis\n",
    "info_analysis = analyze_token_information(result['token_probabilities'])\n",
    "\n",
    "print(\"Information-Theoretic Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Mean Surprisal (Average Entropy): {info_analysis['mean_surprisal']:.4f} bits\")\n",
    "print(f\"Total Information Content:        {info_analysis['total_information']:.4f} bits\")\n",
    "print(f\"Perplexity:                       {info_analysis['perplexity']:.4f}\")\n",
    "print(f\"Min Surprisal (most predictable): {info_analysis['min_surprisal']:.4f} bits\")\n",
    "print(f\"Max Surprisal (most surprising):  {info_analysis['max_surprisal']:.4f} bits\")\n",
    "print(f\"Std Dev of Surprisal:             {info_analysis['std_surprisal']:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb69e27",
   "metadata": {},
   "source": [
    "### Visualize Information Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b89165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Token probabilities\n",
    "ax1 = axes[0]\n",
    "positions = range(1, len(result['token_probabilities']) + 1)\n",
    "ax1.plot(positions, result['token_probabilities'], marker='o', linewidth=2, markersize=6, color='steelblue')\n",
    "ax1.set_xlabel('Token Position', fontsize=12)\n",
    "ax1.set_ylabel('Probability', fontsize=12)\n",
    "ax1.set_title('Token Probabilities Across Generation', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Plot 2: Surprisal (Shannon Information)\n",
    "ax2 = axes[1]\n",
    "surprisals = info_analysis['surprisals']\n",
    "colors = ['green' if s < info_analysis['mean_surprisal'] else 'orange' if s < info_analysis['mean_surprisal'] + info_analysis['std_surprisal'] else 'red' for s in surprisals]\n",
    "ax2.bar(positions, surprisals, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=info_analysis['mean_surprisal'], color='blue', linestyle='--', linewidth=2, label='Mean Surprisal')\n",
    "ax2.set_xlabel('Token Position', fontsize=12)\n",
    "ax2.set_ylabel('Surprisal (bits)', fontsize=12)\n",
    "ax2.set_title('Shannon Information (Surprisal) per Token', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create token table\n",
    "df = pd.DataFrame({\n",
    "    'Position': positions,\n",
    "    'Token': [t.replace(' ', '␣') for t in result['generated_tokens']],\n",
    "    'Probability': result['token_probabilities'],\n",
    "    'Surprisal (bits)': surprisals\n",
    "})\n",
    "\n",
    "print(\"\\nDetailed Token Information:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3820f",
   "metadata": {},
   "source": [
    "## 5. Intervention: Modifying Generation\n",
    "\n",
    "Now let's intervene in the generation process. We can:\n",
    "- Force specific tokens at certain positions\n",
    "- Boost or suppress certain token probabilities\n",
    "- Apply custom transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize intervention manager\n",
    "intervention_manager = InterventionManager(extractor)\n",
    "\n",
    "print(\"✓ Intervention Manager initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e1c41",
   "metadata": {},
   "source": [
    "### Example 1: Force Specific Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_prompt = \"The best programming language is\"\n",
    "\n",
    "# Force specific tokens at positions 0 and 2\n",
    "forced_tokens = {\n",
    "    0: \" Python\",  # Force first token to be \"Python\"\n",
    "    2: \" its\"      # Force third token\n",
    "}\n",
    "\n",
    "result_forced = intervention_manager.generate_with_intervention(\n",
    "    prompt=intervention_prompt,\n",
    "    max_new_tokens=20,\n",
    "    forced_tokens=forced_tokens,\n",
    "    temperature=0.8,\n",
    "    track_alternatives=True\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {intervention_prompt}\")\n",
    "print(f\"\\nGenerated with forced tokens: {result_forced['generated_text']}\\n\")\n",
    "print(\"Intervention Log:\")\n",
    "print(\"-\" * 80)\n",
    "for entry in result_forced['intervention_log'][:10]:\n",
    "    if entry['intervention']:\n",
    "        alt = entry.get('alternative_token', 'N/A')\n",
    "        alt_prob = entry.get('alternative_probability', 0)\n",
    "        print(f\"Position {entry['position']}: FORCED '{entry['token']}' (p={entry['probability']:.4f})\")\n",
    "        print(f\"  → Would have been: '{alt}' (p={alt_prob:.4f})\")\n",
    "    else:\n",
    "        print(f\"Position {entry['position']}: '{entry['token']}' (p={entry['probability']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4c303",
   "metadata": {},
   "source": [
    "### Example 2: Boost Certain Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa66b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an intervention that boosts science-related words\n",
    "science_boost = intervention_manager.create_token_boost_intervention(\n",
    "    boost_tokens=[' science', ' scientific', ' research', ' study', ' theory'],\n",
    "    boost_factor=3.0\n",
    ")\n",
    "\n",
    "prompt_science = \"The most important discovery was\"\n",
    "\n",
    "# Generate with boost\n",
    "result_boosted = intervention_manager.generate_with_intervention(\n",
    "    prompt=prompt_science,\n",
    "    max_new_tokens=25,\n",
    "    intervention_fn=science_boost,\n",
    "    temperature=0.9,\n",
    "    track_alternatives=True\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt_science}\")\n",
    "print(f\"\\nGenerated with science boost:\\n{result_boosted['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af06340b",
   "metadata": {},
   "source": [
    "### Example 3: Suppress Certain Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an intervention that suppresses common words\n",
    "suppress_common = intervention_manager.create_token_suppression_intervention(\n",
    "    suppress_tokens=[' the', ' a', ' an', ' is', ' was', ' are'],\n",
    "    suppression_strength=0.1\n",
    ")\n",
    "\n",
    "prompt_suppress = \"Once upon a time there\"\n",
    "\n",
    "result_suppressed = intervention_manager.generate_with_intervention(\n",
    "    prompt=prompt_suppress,\n",
    "    max_new_tokens=20,\n",
    "    intervention_fn=suppress_common,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt_suppress}\")\n",
    "print(f\"\\nGenerated with common word suppression:\\n{result_suppressed['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53da4b4d",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis: Intervention vs. Baseline\n",
    "\n",
    "Let's compare the information-theoretic properties of intervened vs. baseline generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9887f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze intervention effects\n",
    "analysis = intervention_manager.analyze_intervention_effects(\n",
    "    prompt=\"Artificial intelligence will\",\n",
    "    intervention_fn=science_boost,\n",
    "    max_new_tokens=15,\n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "print(\"Comparative Analysis: Intervention vs Baseline\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Mean Perplexity (Intervention): {analysis['mean_intervention_perplexity']:.4f}\")\n",
    "print(f\"Mean Perplexity (Baseline):     {analysis['mean_baseline_perplexity']:.4f}\")\n",
    "print(f\"\\nMean Surprisal (Intervention):  {analysis['mean_intervention_entropy']:.4f} bits\")\n",
    "print(f\"Mean Surprisal (Baseline):       {analysis['mean_baseline_entropy']:.4f} bits\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"\\nSample Generations (Intervention):\")\n",
    "for i, sample in enumerate(analysis['intervention_samples'], 1):\n",
    "    print(f\"{i}. {sample['generated_text'][:100]}...\")\n",
    "\n",
    "print(\"\\nSample Generations (Baseline):\")\n",
    "for i, sample in enumerate(analysis['baseline_samples'], 1):\n",
    "    print(f\"{i}. {sample['generated_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda2272",
   "metadata": {},
   "source": [
    "## 7. Custom Intervention Example\n",
    "\n",
    "Create your own custom intervention function. Here's an example that increases entropy (randomness) for more diverse outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom intervention: Entropy maximization\n",
    "def increase_diversity(logits, position, context):\n",
    "    \"\"\"\n",
    "    Increase diversity by flattening the probability distribution.\n",
    "    \"\"\"\n",
    "    # Apply higher temperature to increase entropy\n",
    "    temperature = 1.5\n",
    "    return logits / temperature\n",
    "\n",
    "# Test the custom intervention\n",
    "prompt_custom = \"The future of technology includes\"\n",
    "\n",
    "result_custom = intervention_manager.generate_with_intervention(\n",
    "    prompt=prompt_custom,\n",
    "    max_new_tokens=25,\n",
    "    intervention_fn=increase_diversity,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt_custom}\")\n",
    "print(f\"\\nGenerated with diversity boost:\\n{result_custom['generated_text']}\")\n",
    "\n",
    "# Analyze information content\n",
    "custom_info = analyze_token_information(result_custom['token_probabilities'])\n",
    "print(f\"\\nMean surprisal: {custom_info['mean_surprisal']:.4f} bits\")\n",
    "print(f\"Perplexity: {custom_info['perplexity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e7f045",
   "metadata": {},
   "source": [
    "## 8. Advanced: Conditional Interventions\n",
    "\n",
    "Apply different interventions based on conditions during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaeb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conditional intervention: boost science words only in first 10 tokens\n",
    "def first_10_positions(position, context):\n",
    "    return position < 10\n",
    "\n",
    "conditional_intervention = intervention_manager.create_conditional_intervention(\n",
    "    condition_fn=first_10_positions,\n",
    "    true_intervention=science_boost,\n",
    "    false_intervention=None  # No intervention after position 10\n",
    ")\n",
    "\n",
    "result_conditional = intervention_manager.generate_with_intervention(\n",
    "    prompt=\"In the beginning,\",\n",
    "    max_new_tokens=20,\n",
    "    intervention_fn=conditional_intervention,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(\"Generated with conditional intervention (boost first 10 tokens only):\")\n",
    "print(result_conditional['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de73a7",
   "metadata": {},
   "source": [
    "## 9. Experiment: Your Turn!\n",
    "\n",
    "Now it's your turn to experiment. Try:\n",
    "- Different prompts\n",
    "- Different intervention strategies\n",
    "- Analyzing the information-theoretic properties\n",
    "- Creating custom intervention functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR EXPERIMENTS HERE\n",
    "your_prompt = \"Enter your prompt here\"\n",
    "\n",
    "# Example: Generate and analyze\n",
    "# your_result = extractor.generate_with_probabilities(\n",
    "#     prompt=your_prompt,\n",
    "#     max_new_tokens=30,\n",
    "#     temperature=0.8\n",
    "# )\n",
    "\n",
    "# your_info = analyze_token_information(your_result['token_probabilities'])\n",
    "# print(your_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea7e41",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ **Probability Extraction**: Accessed decoder probabilities for each token\n",
    "2. ✅ **Intervention**: Modified generation process with various strategies\n",
    "3. ✅ **Information Theory**: Computed entropy and Shannon information\n",
    "4. ✅ **Visualization**: Created insightful plots of token probabilities and surprisal\n",
    "5. ✅ **Comparative Analysis**: Compared intervened vs baseline generation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different models\n",
    "- Try more complex intervention strategies\n",
    "- Analyze longer sequences\n",
    "- Export results for further analysis\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Source code: `../src/`\n",
    "- Documentation: `../README.md`\n",
    "- Examples: `../examples/`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
